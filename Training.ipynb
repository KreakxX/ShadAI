{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164857e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57270d54",
   "metadata": {},
   "source": [
    " Goal:\n",
    "\n",
    "- Build a Encoder - Decoder Model with attention LSTM (little brother of the transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d56ad4",
   "metadata": {},
   "source": [
    "Roadmap:\n",
    "\n",
    "- Tokenizer: Inputs the text tokenizes it to token IDs like [152323,11]\n",
    "-  Embedding Layer: Takes the token Ids and maps them to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e299ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation\n",
    "input_texts = []\n",
    "output_texts = []\n",
    "\n",
    "\n",
    "dataset_path = \"/Dataset\"\n",
    "code_path = os.path.join(dataset_path,\"Code\")\n",
    "prompt_path = os.path.join(dataset_path,\"prompts\")\n",
    "\n",
    "\n",
    "# Dataset Paths\n",
    "\n",
    "\n",
    "prompt_files = sorted(os.listdir(prompt_path))  \n",
    "code_files = sorted(os.listdir(code_path))\n",
    "\n",
    "\n",
    "# Sorts all files correctly to match like 0.txt , 1.txt, 2.txt\n",
    "\n",
    "for p_file, c_file in zip(prompt_files,code_files):\n",
    "  with open(os.path.join(prompt_path,p_file), \"r\", encoding=\"utf-8\") as f:\n",
    "    prompt_text = f.read().strip()\n",
    "  with open(os.path.join(code_path,c_file),\"r\", encdoing=\"utf-8\") as f:\n",
    "    code_text = f.read().strip()\n",
    "\n",
    "\n",
    "# Open each file and reads the Content\n",
    "\n",
    "input_texts.append(prompt_text)\n",
    "output_texts.append(code_text)\n",
    "\n",
    "\n",
    "# Appends the content to the Dataset Arrays\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa390d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m\n\u001b[0;32m     19\u001b[0m decoder_sequences \u001b[38;5;241m=\u001b[39m output_tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<Start> \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m t \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<End>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m output_texts])\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# So the text_to_sequences is therefore to convert each word or also called Token to its corresponding ID\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# In this Case we first train the Tokenizers to generate IDs, and than we let the Tokenizers generate IDs\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m max_encoder_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mencoder_sequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m max_decoder_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m decoder_sequences)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# These are the neccessary lengths so we can pad all sequences in a batch to have the same size this process is calling padding \u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# So we get the max length and afterwards check if the length of sequence is long enough else we add zero's till the max_encoder / max_decoder length is reached\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "\n",
    "input_tokenizer = Tokenizer(num_words= 10000, oov_token=\"<OOV>\")\n",
    "output_tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "\n",
    "\n",
    "# So the Tokenizer converts words to a Token ID\n",
    "# We have two Tokenizers one for Input taking the Prompt in this Case: \"Button, blue-500, round, \"Click me\" \"\n",
    "# And we have one for Outputing the Answer of the Model in this case <Button classname=\"\" ... > and so on\n",
    "# num words is the ammount of the Top words we keep for the Tokenizer\n",
    "\n",
    "\n",
    "input_tokenizer.fit_on_texts(texts=input_texts)\n",
    "output_tokenizer.fit_on_texts([\"<Start> \" + t + \"<End>\" for t in output_texts])\n",
    "\n",
    "# Training the Tokenizer on the vocab and adding Start and End Tokens\n",
    "\n",
    "encoder_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "decoder_sequences = output_tokenizer.texts_to_sequences([\"<Start> \" + t + \"<End>\" for t in output_texts])\n",
    "\n",
    "\n",
    "# So the text_to_sequences is therefore to convert each word or also called Token to its corresponding ID\n",
    "# In this Case we first train the Tokenizers to generate IDs, and than we let the Tokenizers generate IDs\n",
    "\n",
    "max_encoder_len = max(len(seq) for seq in encoder_sequences)\n",
    "max_decoder_len = max(len(seq) for seq in decoder_sequences)\n",
    "\n",
    "# These are the neccessary lengths so we can pad all sequences in a batch to have the same size this process is calling padding \n",
    "# So we get the max length and afterwards check if the length of sequence is long enough else we add zero's till the max_encoder / max_decoder length is reached\n",
    "\n",
    "encoder_sequences = pad_sequences(encoder_sequences,maxlen=max_encoder_len,padding='post')\n",
    "decoder_sequences = pad_sequences(decoder_sequences,maxlen=max_decoder_len,padding='post')\n",
    "\n",
    "# Padding Done which means all Sequences have the same length now and can be fed to the model\n",
    "\n",
    "decoder_input = decoder_sequences[:,:-1] \n",
    "decoder_target = decoder_sequences[:,:1]\n",
    "\n",
    "# Decoder Input is the input we fed to the Decoder during Training (<Start> + tokens)\n",
    "# And the Decoder Output is the target the Model should predict (tokens + <End>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0104c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model no attention\n",
    "\n",
    "embed_dim = 128  \n",
    "lstm_units = 256\n",
    "\n",
    "input_vocab_size = len(input_tokenizer.word_index) +1\n",
    "output_vocab_size = len(output_tokenizer.word_index) +1\n",
    "\n",
    "\n",
    "# embed_dim is the size of word embeddings\n",
    "# lstm_units is the number of Long short term memory hidden units\n",
    "# and vocab sizes\n",
    "\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,))  \n",
    "\n",
    "# Input is a Sequence of word IDs\n",
    "\n",
    "x = layers.Embedding(input_vocab_size,embed_dim)(encoder_inputs)\n",
    "enocder_outputs, state_h, state_c = layers.LSTM(lstm_units,return_state=True)(x)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Embedding Layer transforms Token IDs to vectors\n",
    "# Encoder_states are states so the decoder knows about what the encoder talked about\n",
    "# LSTM Layers reads the sequence step by step -> returning states that contain all the neccessary information for the Decoder\n",
    "\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,))\n",
    "\n",
    "# Input are the target Tokens shifted\n",
    "\n",
    "y = layers.Embedding(output_vocab_size, embed_dim)(decoder_inputs)\n",
    "decoder_lstm = layers.LSTM(lstm_units, return_sequences=True,return_state=True)\n",
    "decoder_outputs, _, _= decoder_lstm(y,initial_state=encoder_states)\n",
    "decoder_dense = layers.Dense(output_vocab_size,activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Embedding Layer transforms Token IDs to vectors\n",
    "# Now the LSTM layer reads the sequence and starts with the inital states provided by the encoder\n",
    "# Than in the Dense Layer it predicts the words of the vocab with the most probability\n",
    "\n",
    "\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7500eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training and Compiling\n",
    "\n",
    "\n",
    "model.compile(\n",
    "  optimizer=\"adam\",\n",
    "  loss=\"sparse_categorical_crossentropy\",\n",
    "  metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Compile the Model with Adam as optimizer and use the accuracy for training metric\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    [encoder_sequences, decoder_input],\n",
    "    decoder_target,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b41d09",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "- Encoder processes the input sequenc and creates states\n",
    "\n",
    "- Decoder starts processing with inital States from the Encoder\n",
    " -> outputs Predictions for each Target Token\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
