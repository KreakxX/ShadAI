{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7261faea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.utils import pad_sequences\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras import mixed_precision\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1818f5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3060, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "# GPU Settings\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Conda activate tf-gpu to active the GPU envrioment\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "else:\n",
    "    print(\"Bruh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e3c3a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "input_texts = []\n",
    "output_texts = []\n",
    "\n",
    "with open(\"Login_Form_Dataset_2_5m/code.txt\") as f:\n",
    "  code_text = f.read()\n",
    "\n",
    "card_blocks = re.findall(r'(<Card[\\s\\S]*?</Card>)', code_text)\n",
    "\n",
    "output_texts = card_blocks\n",
    "\n",
    "with open(\"Login_Form_Dataset_2_5m/prompts.txt\")as f :\n",
    "  input_texts = f.read().splitlines()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8939991c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens in dataset: 546\n",
      "147\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = Tokenizer(num_words= 500, oov_token=\"<OOV>\", char_level=False,split=\" \",lower=False,filters=\"\")\n",
    "\n",
    "# So the Tokenizer converts words to a Token ID\n",
    "# We have two Tokenizers one for Input taking the Prompt in this Case: \"Button, blue-500, round, \"Click me\" \"\n",
    "# And we have one for Outputing the Answer of the Model in this case <Button classname=\"\" ... > and so on\n",
    "# num words is the ammount of the Top words we keep for the Tokenizer\n",
    "\n",
    "combined_texts = []\n",
    "for prompt, code in zip(input_texts, output_texts):\n",
    "    combined_text = \"<Start> \" + prompt + \" <SEP> \" + code + \" <End>\"\n",
    "    combined_texts.append(combined_text)\n",
    "\n",
    "tokenizer.fit_on_texts(combined_texts)\n",
    "tokenizer.word_index = {\"<PAD>\": 0, **{k: v+1 for k, v in tokenizer.word_index.items()}}\n",
    "tokenizer.index_word = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "print(\"Total unique tokens in dataset:\", len(tokenizer.word_index))  \n",
    "\n",
    "# Training the Tokenizer on the vocab and adding Start and End Tokens\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "    \n",
    "sequences = tokenizer.texts_to_sequences(combined_texts)\n",
    "\n",
    "\n",
    "# So the text_to_sequences is therefore to convert each word or also called Token to its corresponding ID\n",
    "# In this Case we first train the Tokenizers to generate IDs, and than we let the Tokenizers generate IDs\n",
    "\n",
    "max_seq_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "# These are the neccessary lengths so we can pad all sequences in a batch to have the same size this process is calling padding \n",
    "# So we get the max length and afterwards check if the length of sequence is long enough else we add zero's till the max_encoder / max_decoder length is reached\n",
    "\n",
    "sequences = pad_sequences(sequences,maxlen=max_seq_len,padding='post',truncating=\"post\")\n",
    "print(max_seq_len)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99c10657",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 384     \n",
    "num_heads = 12          \n",
    "ff_dim = 1536       \n",
    "num_layers = 6\n",
    "\n",
    "\n",
    "#7813/7813 [==============================] - 2418s 309ms/step - loss: inf - accuracy: 0.8041 - val_loss: 0.7261 - val_accuracy: 0.7909 - lr: 0.0010\n",
    "\n",
    "\n",
    "inputs = layers.Input(shape=(max_seq_len-1))\n",
    "\n",
    "embeddinglayer = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim,mask_zero=True,name=\"embedding_layer\")(inputs)\n",
    "\n",
    "positional_encoding = layers.Embedding(input_dim=max_seq_len-1,output_dim=embed_dim,name=\"positional_encoding_layer\")(tf.range(start=0, limit=max_seq_len-1,delta=1))\n",
    "\n",
    "x = embeddinglayer + positional_encoding\n",
    "\n",
    "# Only Self attention no encoder decoder attention\n",
    "for i in range(num_layers):\n",
    "  self_attention = layers.MultiHeadAttention(\n",
    "  num_heads = num_heads,\n",
    "  key_dim= embed_dim // num_heads,\n",
    "  dropout=0.1,\n",
    "  name=f\"masked_self_attention_{i}\"\n",
    "  )(\n",
    "    query=x,\n",
    "    value=x,\n",
    "    key=x,\n",
    "    use_causal_mask=True   \n",
    "  )\n",
    "    \n",
    "  x1 = layers.LayerNormalization(epsilon=1e-6)(x + self_attention) # residual connection by addiding the attention scores to the previous ones plus semantic context\n",
    "\n",
    "  ffn = tf.keras.Sequential([\n",
    "    layers.Dense(ff_dim,activation=\"gelu\"),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(embed_dim)\n",
    "  ], name=f\"feed_forward_{i}\")\n",
    "\n",
    "  ffn_output = ffn(x1)\n",
    "\n",
    "  x = layers.LayerNormalization(epsilon=1e-6)(ffn_output + x1)\n",
    "\n",
    "outputs = layers.Dense(\n",
    "  vocab_size,\n",
    "  activation=\"softmax\",\n",
    "  name=\"decoder_output_dense\"\n",
    ")(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab4cfdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder_only_transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 146)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_layer (Embedding)    (None, 146, 384)     210048      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 146, 384)    0           ['embedding_layer[0][0]']        \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " masked_self_attention_0 (Multi  (None, 146, 384)    591360      ['tf.__operators__.add[0][0]',   \n",
      " HeadAttention)                                                   'tf.__operators__.add[0][0]',   \n",
      "                                                                  'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 146, 384)    0           ['tf.__operators__.add[0][0]',   \n",
      " mbda)                                                            'masked_self_attention_0[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 146, 384)    768         ['tf.__operators__.add_1[0][0]'] \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " feed_forward_0 (Sequential)    (None, 146, 384)     1181568     ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 146, 384)    0           ['feed_forward_0[0][0]',         \n",
      " mbda)                                                            'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 146, 384)    768         ['tf.__operators__.add_2[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " masked_self_attention_1 (Multi  (None, 146, 384)    591360      ['layer_normalization_1[0][0]',  \n",
      " HeadAttention)                                                   'layer_normalization_1[0][0]',  \n",
      "                                                                  'layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 146, 384)    0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'masked_self_attention_1[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 146, 384)    768         ['tf.__operators__.add_3[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " feed_forward_1 (Sequential)    (None, 146, 384)     1181568     ['layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 146, 384)    0           ['feed_forward_1[0][0]',         \n",
      " mbda)                                                            'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 146, 384)    768         ['tf.__operators__.add_4[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " masked_self_attention_2 (Multi  (None, 146, 384)    591360      ['layer_normalization_3[0][0]',  \n",
      " HeadAttention)                                                   'layer_normalization_3[0][0]',  \n",
      "                                                                  'layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 146, 384)    0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'masked_self_attention_2[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 146, 384)    768         ['tf.__operators__.add_5[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " feed_forward_2 (Sequential)    (None, 146, 384)     1181568     ['layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 146, 384)    0           ['feed_forward_2[0][0]',         \n",
      " mbda)                                                            'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 146, 384)    768         ['tf.__operators__.add_6[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " masked_self_attention_3 (Multi  (None, 146, 384)    591360      ['layer_normalization_5[0][0]',  \n",
      " HeadAttention)                                                   'layer_normalization_5[0][0]',  \n",
      "                                                                  'layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 146, 384)    0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'masked_self_attention_3[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 146, 384)    768         ['tf.__operators__.add_7[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " feed_forward_3 (Sequential)    (None, 146, 384)     1181568     ['layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TFOpLa  (None, 146, 384)    0           ['feed_forward_3[0][0]',         \n",
      " mbda)                                                            'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 146, 384)    768         ['tf.__operators__.add_8[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " masked_self_attention_4 (Multi  (None, 146, 384)    591360      ['layer_normalization_7[0][0]',  \n",
      " HeadAttention)                                                   'layer_normalization_7[0][0]',  \n",
      "                                                                  'layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TFOpLa  (None, 146, 384)    0           ['layer_normalization_7[0][0]',  \n",
      " mbda)                                                            'masked_self_attention_4[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 146, 384)    768         ['tf.__operators__.add_9[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " feed_forward_4 (Sequential)    (None, 146, 384)     1181568     ['layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (TFOpL  (None, 146, 384)    0           ['feed_forward_4[0][0]',         \n",
      " ambda)                                                           'layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 146, 384)    768         ['tf.__operators__.add_10[0][0]']\n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " masked_self_attention_5 (Multi  (None, 146, 384)    591360      ['layer_normalization_9[0][0]',  \n",
      " HeadAttention)                                                   'layer_normalization_9[0][0]',  \n",
      "                                                                  'layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (TFOpL  (None, 146, 384)    0           ['layer_normalization_9[0][0]',  \n",
      " ambda)                                                           'masked_self_attention_5[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 146, 384)    768         ['tf.__operators__.add_11[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " feed_forward_5 (Sequential)    (None, 146, 384)     1181568     ['layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_12 (TFOpL  (None, 146, 384)    0           ['feed_forward_5[0][0]',         \n",
      " ambda)                                                           'layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 146, 384)    768         ['tf.__operators__.add_12[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " decoder_output_dense (Dense)   (None, 146, 547)     210595      ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,067,427\n",
      "Trainable params: 11,067,427\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "592/592 [==============================] - ETA: 0s - loss: 0.4094 - accuracy: 0.8606\n",
      "Epoch 1: val_loss improved from inf to 0.26841, saving model to Models\\LoginFormModel.keras\n",
      "592/592 [==============================] - 109s 172ms/step - loss: 0.4094 - accuracy: 0.8606 - val_loss: 0.2684 - val_accuracy: 0.8868 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"decoder_only_transformer\")\n",
    "\n",
    "initial_learning_rate = 1e-4\n",
    "lr_schedule = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    alpha=0.1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,  \n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='Models/LoginFormModel.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    keras.callbacks.LambdaCallback(\n",
    "        on_batch_end=lambda batch, logs: tf.clip_by_global_norm([v for v in model.trainable_variables], 1.0)\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "X = sequences[:, :-1]  \n",
    "y = sequences[:, 1:]   \n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "validation_split = 0.2 \n",
    "\n",
    "history = model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    batch_size=batch_size,\n",
    "    epochs=1,\n",
    "    validation_split=validation_split,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "model.save(\"Models/LoginFormModel.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6d0e1fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(result)\n\u001b[0;32m     26\u001b[0m test_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild a user sign in component with social login options\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 27\u001b[0m generated \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_button_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Button Code:\u001b[39m\u001b[38;5;124m\"\u001b[39m, generated)\n",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m, in \u001b[0;36mgenerate_button_code\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     10\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):  \n\u001b[1;32m---> 12\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_transformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(input_sequence) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(generated_tokens) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m next_token \u001b[38;5;241m==\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mword_index\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<End>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m next_token \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py:2253\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[0;32m   2252\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 2253\u001b[0m     tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   2255\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test Model\n",
    "\n",
    "decoder_transformer = keras.models.load_model('Models/LoginFormModel.keras')\n",
    "def generate_button_code(prompt):\n",
    "    prompt = \"<Start> \" + prompt + \" <SEP>\"\n",
    "    \n",
    "    input_sequence = tokenizer.texts_to_sequences([prompt])[0]\n",
    "    current_sequence = pad_sequences([input_sequence], maxlen=max_seq_len-1, padding='post')\n",
    "    \n",
    "    generated_tokens = []\n",
    "    for _ in range(200):  \n",
    "        predictions = decoder_transformer.predict(current_sequence, verbose=0)\n",
    "        next_token = np.argmax(predictions[0, len(input_sequence) + len(generated_tokens) - 1])\n",
    "        \n",
    "        if next_token == tokenizer.word_index.get(\"<End>\", 0) or next_token == 0:\n",
    "            break\n",
    "            \n",
    "        generated_tokens.append(next_token)\n",
    "        \n",
    "        new_sequence = input_sequence + generated_tokens\n",
    "        current_sequence = pad_sequences([new_sequence], maxlen=max_seq_len-1, padding='post')\n",
    "    \n",
    "    result = [tokenizer.index_word.get(token, \"\") for token in generated_tokens]\n",
    "    return \" \".join(result)\n",
    "\n",
    "test_prompt = \"Build a user sign in component with social login options\"\n",
    "generated = generate_button_code(test_prompt)\n",
    "print(\"Generated Button Code:\", generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc14742c",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- first Padding with Tokenizer map it 0 good Fix \n",
    "- Decoder Only model is very powerful, but if too big == no good results\n",
    "- Dataset Quality is the second Important Part\n",
    "\n",
    "# Further Improvements\n",
    "\n",
    "- Finetuning the Model on specific Problems \n",
    "- More Data with more variation esspecially with the Prompt input\n",
    "\n",
    "\n",
    "# Next Steps\n",
    "\n",
    "- One Model for everything \n",
    "- with better login Form Dataset and better GPU optimization "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
