{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7261faea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.utils import pad_sequences\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Conda activate tf-gpu to active the GPU envrioment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3e3c3a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "\n",
    "input_texts = []\n",
    "output_texts = []\n",
    "\n",
    "\n",
    "with open(\"Buttons/prompts.txt\") as f:\n",
    "  input_texts = f.read().splitlines()\n",
    "\n",
    "with open(\"Buttons/code.txt\") as f:\n",
    "  output_texts = f.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8939991c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words= 20000, oov_token=\"<OOV>\", char_level=False,split=\" \",lower=False,filters=\"\")\n",
    "\n",
    "# So the Tokenizer converts words to a Token ID\n",
    "# We have two Tokenizers one for Input taking the Prompt in this Case: \"Button, blue-500, round, \"Click me\" \"\n",
    "# And we have one for Outputing the Answer of the Model in this case <Button classname=\"\" ... > and so on\n",
    "# num words is the ammount of the Top words we keep for the Tokenizer\n",
    "\n",
    "combined_texts = []\n",
    "for prompt, code in zip(input_texts, output_texts):\n",
    "    combined_text = \"<Start> \" + prompt + \" <SEP> \" + code + \" <End>\"\n",
    "    combined_texts.append(combined_text)\n",
    "\n",
    "tokenizer.fit_on_texts(combined_texts)\n",
    "tokenizer.word_index = {\"<PAD>\": 0, **{k: v+1 for k, v in tokenizer.word_index.items()}}\n",
    "tokenizer.index_word = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "# Training the Tokenizer on the vocab and adding Start and End Tokens\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(combined_texts)\n",
    "\n",
    "\n",
    "# So the text_to_sequences is therefore to convert each word or also called Token to its corresponding ID\n",
    "# In this Case we first train the Tokenizers to generate IDs, and than we let the Tokenizers generate IDs\n",
    "\n",
    "max_seq_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "# These are the neccessary lengths so we can pad all sequences in a batch to have the same size this process is calling padding \n",
    "# So we get the max length and afterwards check if the length of sequence is long enough else we add zero's till the max_encoder / max_decoder length is reached\n",
    "\n",
    "sequences = pad_sequences(sequences,maxlen=max_seq_len,padding='post',truncating=\"post\")\n",
    "print(max_seq_len)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "99c10657",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128       \n",
    "num_heads = 4          \n",
    "ff_dim = 512        \n",
    "num_layers = 4\n",
    "\n",
    "#3125/3125 [==============================] - 459s 147ms/step - loss: 0.7670 - accuracy: 0.8050 - val_loss: 0.8075 - val_accuracy: 0.7951 - lr: 0.0010\n",
    "\n",
    "\n",
    "inputs = layers.Input(shape=(max_seq_len-1))\n",
    "\n",
    "embeddinglayer = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim,mask_zero=True)(inputs)\n",
    "\n",
    "positional_encoding = layers.Embedding(input_dim=max_seq_len-1,output_dim=embed_dim)(tf.range(start=0, limit=max_seq_len-1,delta=1))\n",
    "\n",
    "x = embeddinglayer + positional_encoding\n",
    "\n",
    "# Only Self attention no encoder decoder attention\n",
    "for i in range(num_layers):\n",
    "  self_attention = layers.MultiHeadAttention(\n",
    "  num_heads = num_heads,\n",
    "  key_dim= embed_dim // num_heads,\n",
    "  dropout=0.1,\n",
    "  name=f\"masked_self_attention_{i}\"\n",
    "  )(\n",
    "    query=x,\n",
    "    value=x,\n",
    "    key=x,\n",
    "    use_causal_mask=True   \n",
    "  )\n",
    "    \n",
    "  x1 = layers.LayerNormalization(epsilon=1e-6)(x + self_attention) # residual connection by addiding the attention scores to the previous ones plus semantic context\n",
    "\n",
    "  ffn = tf.keras.Sequential([\n",
    "    layers.Dense(ff_dim,activation=\"gelu\"),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(embed_dim)\n",
    "  ], name=f\"feed_forward_{i}\")\n",
    "\n",
    "  ffn_output = ffn(x1)\n",
    "\n",
    "  x = layers.LayerNormalization(epsilon=1e-6)(ffn_output + x1)\n",
    "\n",
    "outputs = layers.Dense(\n",
    "  vocab_size,\n",
    "  activation=\"softmax\",\n",
    "  name=\"decoder_output_dense\"\n",
    ")(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4cfdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder_only_transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None, 32)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_22 (Embedding)       (None, 32, 128)      2039552     ['input_12[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_140 (TFOp  (None, 32, 128)     0           ['embedding_22[0][0]']           \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " masked_self_attention_0 (Multi  (None, 32, 128)     66048       ['tf.__operators__.add_140[0][0]'\n",
      " HeadAttention)                                                  , 'tf.__operators__.add_140[0][0]\n",
      "                                                                 ',                               \n",
      "                                                                  'tf.__operators__.add_140[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_141 (TFOp  (None, 32, 128)     0           ['tf.__operators__.add_140[0][0]'\n",
      " Lambda)                                                         , 'masked_self_attention_0[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_129 (Layer  (None, 32, 128)     256         ['tf.__operators__.add_141[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " feed_forward_0 (Sequential)    (None, 32, 128)      131712      ['layer_normalization_129[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_142 (TFOp  (None, 32, 128)     0           ['feed_forward_0[0][0]',         \n",
      " Lambda)                                                          'layer_normalization_129[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_130 (Layer  (None, 32, 128)     256         ['tf.__operators__.add_142[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " masked_self_attention_1 (Multi  (None, 32, 128)     66048       ['layer_normalization_130[0][0]',\n",
      " HeadAttention)                                                   'layer_normalization_130[0][0]',\n",
      "                                                                  'layer_normalization_130[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_143 (TFOp  (None, 32, 128)     0           ['layer_normalization_130[0][0]',\n",
      " Lambda)                                                          'masked_self_attention_1[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_131 (Layer  (None, 32, 128)     256         ['tf.__operators__.add_143[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " feed_forward_1 (Sequential)    (None, 32, 128)      131712      ['layer_normalization_131[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_144 (TFOp  (None, 32, 128)     0           ['feed_forward_1[0][0]',         \n",
      " Lambda)                                                          'layer_normalization_131[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_132 (Layer  (None, 32, 128)     256         ['tf.__operators__.add_144[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " masked_self_attention_2 (Multi  (None, 32, 128)     66048       ['layer_normalization_132[0][0]',\n",
      " HeadAttention)                                                   'layer_normalization_132[0][0]',\n",
      "                                                                  'layer_normalization_132[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_145 (TFOp  (None, 32, 128)     0           ['layer_normalization_132[0][0]',\n",
      " Lambda)                                                          'masked_self_attention_2[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_133 (Layer  (None, 32, 128)     256         ['tf.__operators__.add_145[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " feed_forward_2 (Sequential)    (None, 32, 128)      131712      ['layer_normalization_133[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_146 (TFOp  (None, 32, 128)     0           ['feed_forward_2[0][0]',         \n",
      " Lambda)                                                          'layer_normalization_133[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_134 (Layer  (None, 32, 128)     256         ['tf.__operators__.add_146[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " masked_self_attention_3 (Multi  (None, 32, 128)     66048       ['layer_normalization_134[0][0]',\n",
      " HeadAttention)                                                   'layer_normalization_134[0][0]',\n",
      "                                                                  'layer_normalization_134[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_147 (TFOp  (None, 32, 128)     0           ['layer_normalization_134[0][0]',\n",
      " Lambda)                                                          'masked_self_attention_3[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_135 (Layer  (None, 32, 128)     256         ['tf.__operators__.add_147[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " feed_forward_3 (Sequential)    (None, 32, 128)      131712      ['layer_normalization_135[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_148 (TFOp  (None, 32, 128)     0           ['feed_forward_3[0][0]',         \n",
      " Lambda)                                                          'layer_normalization_135[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_136 (Layer  (None, 32, 128)     256         ['tf.__operators__.add_148[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " decoder_output_dense (Dense)   (None, 32, 15934)    2055486     ['layer_normalization_136[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,888,126\n",
      "Trainable params: 4,888,126\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "2465/3125 [======================>.......] - ETA: 1:28 - loss: 1.0849 - accuracy: 0.7740"
     ]
    }
   ],
   "source": [
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"decoder_only_transformer\")\n",
    "\n",
    "initial_learning_rate = 1e-4\n",
    "lr_schedule = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    alpha=0.1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,  \n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_decoder_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    keras.callbacks.LambdaCallback(\n",
    "        on_batch_end=lambda batch, logs: tf.clip_by_global_norm([v for v in model.trainable_variables], 1.0)\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "X = sequences[:, :-1]  \n",
    "y = sequences[:, 1:]   \n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "validation_split = 0.2 \n",
    "\n",
    "history = model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    batch_size=batch_size,\n",
    "    epochs=20,\n",
    "    validation_split=validation_split,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "model.save(\"Decoder_Transforer.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56efa512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference: Not Neccessary\n",
    "# inputs_inf = layers.Input(shape=(max_seq_len))\n",
    "\n",
    "# embeddinglayer_inf = layers.Embedding(input_dim=vocab_size,output_dim=embed_dim, mask_zero=True)(inputs_inf)\n",
    "\n",
    "# positional_encoding_inf = layers.Embedding(input_dim=max_seq_len,output_dim=embed_dim,mask_zero=True)(tf.range(start=0, limit=max_seq_len,delta=1))\n",
    "\n",
    "# x_inf = embeddinglayer_inf + positional_encoding_inf\n",
    "\n",
    "# for i in range(num_layers):\n",
    "#   self_attention_inf = layers.MultiHeadAttention(\n",
    "#     num_heads=num_heads,\n",
    "#     key_dim= embed_dim//num_heads,\n",
    "#     dropout=0.1\n",
    "#   )(\n",
    "#     query= x_inf,\n",
    "#     value=x_inf,\n",
    "#     key=x_inf,\n",
    "#     use_causal_mask=True   \n",
    "#   )\n",
    "\n",
    "#   x2_inf = layers.LayerNormalization(epsilon=1e-6)(x_inf + self_attention_inf)\n",
    "\n",
    "#   ffn_inf = keras.Sequential([\n",
    "#     layers.Dense(ff_dim,activation=\"gelu\"),\n",
    "#     layers.Dense(ff_dim)\n",
    "#   ])\n",
    "\n",
    "#   ffn_output_inf = ffn_inf(x2_inf)\n",
    "\n",
    "#   x3_inf = layers.LayerNormalization(epsilon=1e-6)(ffn_output_inf + x2_inf)\n",
    "\n",
    "#   outputs = layers.Dense(\n",
    "#   vocab_size,\n",
    "#   activation=\"softmax\",\n",
    "#   name=\"decoder_output_dense\"\n",
    "# )(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d0e1fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"decoder_only_transformer\" is incompatible with the layer: expected shape=(None, 32), found shape=(None, 31)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m current_sequence \u001b[38;5;241m=\u001b[39m input_sequence\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_seq_len):\n\u001b[1;32m---> 10\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_transformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])      \n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m next_token \u001b[38;5;241m==\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mword_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<End>\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filezi65cq47.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Henri\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"decoder_only_transformer\" is incompatible with the layer: expected shape=(None, 32), found shape=(None, 31)\n"
     ]
    }
   ],
   "source": [
    "# Test Model\n",
    "prompt = \"Can you create a Light Yellow button that displays 'Confirm'?\"\n",
    "\n",
    "input_sequence = tokenizer.texts_to_sequences([prompt])\n",
    "max_seq_len = max_seq_len -1\n",
    "input_sequence = pad_sequences(input_sequence,maxlen=max_seq_len,padding='post')\n",
    "\n",
    "decoder_transformer = keras.models.load_model('best_decoder_model.keras')\n",
    "current_sequence = input_sequence.copy()\n",
    "for _ in range(max_seq_len):\n",
    "    predictions = decoder_transformer.predict(current_sequence)\n",
    "    \n",
    "    next_token = np.argmax(predictions[0, -1, :])      \n",
    "    if next_token == tokenizer.word_index[\"<End>\"]:\n",
    "        break\n",
    "\n",
    "    current_sequence = np.append(current_sequence, [[next_token]], axis=1)\n",
    "    if current_sequence.shape[1] > max_seq_len:\n",
    "        current_sequence = current_sequence[:, 1:]\n",
    "\n",
    "original_length = input_sequence.shape[1]\n",
    "generated_tokens = current_sequence[0, original_length:]\n",
    "decoded_words = [tokenizer.index_word.get(i, '') for i in generated_tokens]\n",
    "decoded_sentence = ' '.join(decoded_words)\n",
    "print(decoded_sentence + \"sentence\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
