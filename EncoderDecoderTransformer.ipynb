{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3ac8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.utils import pad_sequences\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Conda activate tf-gpu to active the GPU envrioment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e32a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "\n",
    "input_texts = []\n",
    "output_texts = []\n",
    "\n",
    "\n",
    "with open(\"Buttons/prompts.txt\") as f:\n",
    "  input_texts = f.read().splitlines()\n",
    "\n",
    "with open(\"Buttons/code.txt\") as f:\n",
    "  output_texts = f.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e9800",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Tokenizer\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m input_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizer\u001b[49m(num_words\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3000\u001b[39m, oov_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<OOV>\u001b[39m\u001b[38;5;124m\"\u001b[39m, char_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m,lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m output_tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m, oov_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<OOV>\u001b[39m\u001b[38;5;124m\"\u001b[39m, char_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# So the Tokenizer converts words to a Token ID\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# We have two Tokenizers one for Input taking the Prompt in this Case: \"Button, blue-500, round, \"Click me\" \"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# And we have one for Outputing the Answer of the Model in this case <Button classname=\"\" ... > and so on\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# num words is the ammount of the Top words we keep for the Tokenizer\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "\n",
    "input_tokenizer = Tokenizer(num_words= 3000, oov_token=\"<OOV>\", char_level=False,split=\" \",lower=False,filters=\"\")\n",
    "output_tokenizer = Tokenizer(num_words=20000, oov_token=\"<OOV>\", char_level=False,split=\" \", filters=\"\", lower=False)\n",
    "\n",
    "# So the Tokenizer converts words to a Token ID\n",
    "# We have two Tokenizers one for Input taking the Prompt in this Case: \"Button, blue-500, round, \"Click me\" \"\n",
    "# And we have one for Outputing the Answer of the Model in this case <Button classname=\"\" ... > and so on\n",
    "# num words is the ammount of the Top words we keep for the Tokenizer\n",
    "\n",
    "\n",
    "input_tokenizer.fit_on_texts(texts=input_texts)\n",
    "output_tokenizer.fit_on_texts([\"<Start> \" + t + \" <End>\" for t in output_texts])\n",
    "\n",
    "import pickle\n",
    "with open(\"input_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(input_tokenizer, f)\n",
    "\n",
    "with open(\"output_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(output_tokenizer, f)\n",
    "    \n",
    "print(f\"Actual input vocab: {len(input_tokenizer.word_index)}\")\n",
    "print(f\"Actual output vocab: {len(output_tokenizer.word_index)}\")\n",
    "\n",
    "\n",
    "input_tokenizer.word_index = {\"<PAD>\": 0, **{k: v+1 for k, v in input_tokenizer.word_index.items()}}\n",
    "input_tokenizer.index_word = {v: k for k, v in input_tokenizer.word_index.items()}\n",
    "\n",
    "output_tokenizer.word_index = {\"<PAD>\": 0, **{k: v+1 for k, v in output_tokenizer.word_index.items()}}\n",
    "output_tokenizer.index_word = {v: k for k, v in output_tokenizer.word_index.items()}\n",
    "# Training the Tokenizer on the vocab and adding Start and End Tokens\n",
    "\n",
    "encoder_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "decoder_sequences = output_tokenizer.texts_to_sequences([\"<Start> \" + t + \" <End>\" for t in output_texts])\n",
    "\n",
    "\n",
    "# So the text_to_sequences is therefore to convert each word or also called Token to its corresponding ID\n",
    "# In this Case we first train the Tokenizers to generate IDs, and than we let the Tokenizers generate IDs\n",
    "\n",
    "max_encoder_len = max(len(seq) for seq in encoder_sequences)\n",
    "max_decoder_len = max(len(seq) for seq in decoder_sequences)\n",
    "\n",
    "# These are the neccessary lengths so we can pad all sequences in a batch to have the same size this process is calling padding \n",
    "# So we get the max length and afterwards check if the length of sequence is long enough else we add zero's till the max_encoder / max_decoder length is reached\n",
    "\n",
    "encoder_sequences = pad_sequences(encoder_sequences,maxlen=max_encoder_len,padding='post')\n",
    "decoder_sequences = pad_sequences(decoder_sequences,maxlen=max_decoder_len,padding='post')  # theoretically should improve performance because the context is at teh end so the model learns better\n",
    "\n",
    "# Padding Done which means all Sequences have the same length now and can be fed to the model\n",
    "print(f\"Max encoder len: {max_encoder_len}\")\n",
    "print(f\"Max decoder len: {max_decoder_len}\")\n",
    "print(f\"Average encoder len: {np.mean([len(seq) for seq in encoder_sequences])}\")\n",
    "\n",
    "decoder_input = decoder_sequences[:,:-1] \n",
    "decoder_target = decoder_sequences[:,1:]\n",
    "\n",
    "\n",
    "\n",
    "# Decoder Input is the input we fed to the Decoder during Training (<Start> + tokens)\n",
    "# And the Decoder Output is the target the Model should predict (tokens + <End>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faa3569",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "num_heads = 4  # attention headers in the Transformers\n",
    "ff_dim = 512  # inner Dimension in the Transformers feed - forward network\n",
    "num_layers = 3 # number of transformer/attention layers\n",
    "\n",
    "input_vocab_size = len(input_tokenizer.word_index) +1\n",
    "output_vocab_size = len(output_tokenizer.word_index) +1\n",
    "\n",
    "encoder_inputs = layers.Input(shape=(max_encoder_len,), name=\"encoder_inputs\")\n",
    "decoder_inputs = layers.Input(shape=(max_decoder_len-1,),name=\"decoder_inputs\")\n",
    "\n",
    "encoder_embedding = layers.Embedding(input_dim=input_vocab_size, output_dim=embed_dim, mask_zero=True,name=\"encoder_embedding\")(encoder_inputs)\n",
    "decoder_embedding = layers.Embedding(input_dim=output_vocab_size,output_dim=embed_dim, mask_zero=True, name=\"decoder_embedding\")(decoder_inputs)\n",
    "\n",
    "# Embeding each Tokenized Token gets a Vector with embed_dim as the Dimension\n",
    "# nn only understand Numbers and Vectors are to mark similiarites like blue and red are in the embedding World close together\n",
    "\n",
    "# Positional Encoding first new Thing included in Transformers\n",
    "decoder_seq_len = max_decoder_len -1 # minus one because we train to predict the next token so we always have a start Token\n",
    "encoder_seq_len = max_encoder_len\n",
    "\n",
    "encoder_positions = layers.Embedding(\n",
    "  input_dim=encoder_seq_len,\n",
    "  output_dim=embed_dim,\n",
    "  name=\"encoder_position_embedding\"\n",
    ")(tf.range(start=0, limit=encoder_seq_len, delta=1))  \n",
    "\n",
    "# generates a list of Positions starting from 0 to the max length\n",
    "# and for each position its getting vectorized\n",
    "\n",
    "decoder_positions = layers.Embedding(\n",
    "  input_dim=decoder_seq_len,\n",
    "  output_dim=embed_dim,\n",
    "  name=\"decoder_position_embedding\"\n",
    ")(tf.range(start=0, limit=decoder_seq_len, delta=1))\n",
    "\n",
    "# same for here\n",
    "\n",
    "encoder_pos = encoder_embedding + encoder_positions\n",
    "\n",
    "encoder_output = encoder_pos\n",
    "for i in range(num_layers):\n",
    "  encoder_selfattention = layers.MultiHeadAttention(\n",
    "  num_heads = num_heads,\n",
    "  key_dim= embed_dim // num_heads,\n",
    "  dropout=0.1,\n",
    "  name=f\"encoder_self_attention_{i}\"\n",
    "  )(\n",
    "  query=encoder_output,\n",
    "  value=encoder_output,\n",
    "  key=encoder_output,\n",
    "  )\n",
    "\n",
    "  x0 = layers.LayerNormalization(epsilon=1e-6)(encoder_output + encoder_selfattention)\n",
    "\n",
    "  ffn_encoder = keras.Sequential([\n",
    "  layers.Dense(ff_dim,activation=\"relu\"),\n",
    "  layers.Dense(embed_dim),\n",
    "  ])\n",
    "\n",
    "  ffn_encoder_output = ffn_encoder(x0)\n",
    "\n",
    "  encoder_output = layers.LayerNormalization(epsilon=1e-6)(ffn_encoder_output + x0)\n",
    "\n",
    "# added Self attention to the encoder \n",
    "# with resiudal connections so the Input tokens are all context connected to each other \n",
    "# which improves model performance and decoder training\n",
    "\n",
    "decoder_pos = decoder_embedding + decoder_positions\n",
    "decoder_outputs = decoder_pos\n",
    "# than the Semantic vector and the postion vector getting added togeter for the Full Context / attention vector\n",
    "\n",
    "for i in range(num_layers):\n",
    "  # look_ahead_mask = tf.linalg.band_part(tf.ones((decoder_seq_len, decoder_seq_len)), -1, 0)\n",
    "  \n",
    "  # Creates a Matrix == tf.ones a Matrix of ones with shape of the len \n",
    "  # -1 == num_lower == keep all bottom elemets\n",
    "  # 0 == num_upper = dont keep the upper elemets\n",
    "\n",
    "  # Reason so the Model doesnt look in the Future when Training or predicting\n",
    "\n",
    "  self_attention = layers.MultiHeadAttention(\n",
    "    num_heads = num_heads,\n",
    "    key_dim= embed_dim // num_heads,\n",
    "    dropout=0.1,\n",
    "    name=f\"masked_self_attention_{i}\"\n",
    "  )(\n",
    "    query=decoder_outputs,\n",
    "    value=decoder_outputs,\n",
    "    key=decoder_outputs,\n",
    "    use_causal_mask=True   \n",
    ")\n",
    "\n",
    "  # Self Attention is for calculating the similarity between words sourrounding the current Token\n",
    "\n",
    "  # next we have MultiheadAttention wie 4 heads and the right key diemension and dropout preventing overfitting \n",
    "  # query = actual token\n",
    "  # Key, Value = all tokens in the Sequenz\n",
    "  # with the mask just looks at the ones not a 0 1 == previous/ current tokesn and 0 == further Tokens\n",
    "\n",
    "  x1 = layers.LayerNormalization(epsilon=1e-6)(decoder_outputs + self_attention)\n",
    "  # Residual Connection by adding the semantic and postion Vector to the attention\n",
    "  # Stabilizes Training by normalizing Vectors\n",
    "\n",
    "  encoder_decoder_attention = layers.MultiHeadAttention(\n",
    "    num_heads= num_heads,\n",
    "    key_dim=embed_dim // num_heads,\n",
    "    dropout=0.1,\n",
    "    name=f\"encoder_decoder_attention_{i}\"\n",
    "  )(\n",
    "    query=x1,     # the Decoder tokens with the self attention\n",
    "    value=encoder_output,    # output from encoder with Information and the context of the self attention\n",
    "    key=encoder_output,\n",
    "    attention_mask=None\n",
    "  )\n",
    "\n",
    "  # the effect therefore is that every Decoder Tokens with the self attention which is crucial for \n",
    "  # understanding the sequence and referencing each word to its sourring words, by calculating the self attention\n",
    "  # can now look on the full input Sequence with the encoder_decoder attention\n",
    "\n",
    "  x2 = layers.LayerNormalization(epsilon=1e-6)(x1 + encoder_decoder_attention)\n",
    "  # Now we stabilize again and add to our vector the new Attention also\n",
    "\n",
    "  # Summary x2 contains now the semantic Vector, the Position Vector, the self attention\n",
    "  # (the similiarty to its sourroding words / or the connection between them) and also the encoder_decoder\n",
    "  # attention which means the Context of the Input sequence\n",
    "\n",
    "  ffn = tf.keras.Sequential([\n",
    "    layers.Dense(ff_dim,activation=\"gelu\"),\n",
    "    layers.Dense(embed_dim)\n",
    "  ], name=f\"feed_forward_{i}\")\n",
    "\n",
    "  # Each individual token goes seperately through the FFN\n",
    "  # its for learning non linera comibnations of the Token features\n",
    "\n",
    "  ffn_output = ffn(x2)\n",
    "\n",
    "  # for each Token\n",
    "\n",
    "  decoder_outputs = layers.LayerNormalization(epsilon=1e-6)(x2 + ffn_output)\n",
    "\n",
    "# and we also add everything from before the the fnn output\n",
    "\n",
    "decoder_outputs = layers.Dense(\n",
    "  output_vocab_size,\n",
    "  activation=\"softmax\",\n",
    "  name=\"decoder_output_dense\"\n",
    ")(decoder_outputs)\n",
    "\n",
    "\n",
    "# This is the Dense layer for the Token prediction\n",
    "\n",
    "transformer_model = keras.Model(\n",
    "    inputs=[encoder_inputs, decoder_inputs],\n",
    "    outputs=decoder_outputs,\n",
    "    name=\"transformer_decoder_model\"\n",
    ")\n",
    "\n",
    "transformer_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "earlyStopping = keras.callbacks.EarlyStopping(\n",
    "  monitor=\"val_loss\",\n",
    "  patience=1,\n",
    "  restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "  monitor='val_loss',\n",
    "  factor=0.5,                   \n",
    "  patience=1,\n",
    "  min_lr=1e-6,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "history = transformer_model.fit(\n",
    "    [encoder_sequences, decoder_input],\n",
    "    decoder_target,\n",
    "    batch_size=128,\n",
    "    epochs=2,\n",
    "    validation_split=0.15,\n",
    "    callbacks=[earlyStopping,reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Scheduled Sampling Training - paste at the bottom of your code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3886349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference\n",
    "encoder_model = keras.Model(encoder_inputs,encoder_output, name=\"encoder_model\")\n",
    "\n",
    "\n",
    "decoder_inputs_inf = layers.Input(shape=(1,), name=\"decoder_inputs_inf\")\n",
    "encoder_outputs_input = layers.Input(shape=(max_encoder_len, embed_dim), name=\"encoder_outputs_input\")\n",
    "\n",
    "\n",
    "decoder_embedding_inf = layers.Embedding(\n",
    "    input_dim=output_vocab_size,\n",
    "    output_dim=embed_dim,\n",
    "    mask_zero=True,\n",
    "    name=\"decoder_embedding_inf\"\n",
    ")(decoder_inputs_inf)\n",
    "\n",
    "decoder_position_inf = layers.Embedding(\n",
    "    input_dim=max_decoder_len-1,\n",
    "    output_dim=embed_dim,\n",
    "    name=\"decoder_position_embedding_inf\"\n",
    ")(tf.constant([0]))\n",
    "\n",
    "decoder_pos_inf = decoder_embedding_inf + decoder_position_inf\n",
    "decoder_outputs_inf = decoder_pos_inf\n",
    "for i in range(num_layers):\n",
    "    \n",
    "    self_attention_inf = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=embed_dim // num_heads,\n",
    "        dropout=0.1,\n",
    "        name=f\"masked_self_attention_inf_{i}\"\n",
    "    )(query=decoder_outputs_inf, value=decoder_outputs_inf, key=decoder_outputs_inf, use_causal_mask=True)\n",
    "\n",
    "    x1_inf = layers.LayerNormalization(epsilon=1e-6)(decoder_outputs_inf + self_attention_inf)\n",
    "\n",
    "    enc_dec_attention_inf = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=embed_dim // num_heads,\n",
    "        dropout=0.1,\n",
    "        name=f\"encoder_decoder_attention_inf_{i}\"\n",
    "    )(query=x1_inf, value=encoder_outputs_input, key=encoder_outputs_input)\n",
    "\n",
    "    x2_inf = layers.LayerNormalization(epsilon=1e-6)(x1_inf + enc_dec_attention_inf)\n",
    "\n",
    "    ffn_inf = tf.keras.Sequential([\n",
    "        layers.Dense(ff_dim, activation=\"gelu\"),\n",
    "        layers.Dense(embed_dim)\n",
    "    ], name=f\"feed_forward_inf_{i}\")\n",
    "\n",
    "    ffn_output_inf = ffn_inf(x2_inf)\n",
    "    decoder_outputs_inf = layers.LayerNormalization(epsilon=1e-6)(x2_inf + ffn_output_inf)\n",
    "\n",
    "decoder_outputs_inf = layers.Dense(\n",
    "    output_vocab_size,\n",
    "    activation=\"softmax\",\n",
    "    name=\"decoder_output_dense_inf\"\n",
    ")(decoder_outputs_inf)\n",
    "\n",
    "decoder_model = keras.Model(\n",
    "    inputs=[decoder_inputs_inf, encoder_outputs_input],\n",
    "    outputs=decoder_outputs_inf,\n",
    "    name=\"decoder_inference_model\"\n",
    ")\n",
    "\n",
    "encoder_model.save(\"encoderTransformer.keras\")\n",
    "decoder_model.save(\"decoderTransformer.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122347d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Step\n",
    "\n",
    "encoder_model = keras.models.load_model(\"encoderTransformer.keras\")\n",
    "decoder_model = keras.models.load_model(\"decoderTransformer.keras\")\n",
    "\n",
    "with open(\"input_tokenizer.pkl\", \"rb\") as f:\n",
    "    input_tokenizer = pickle.load(f)\n",
    "\n",
    "with open(\"output_tokenizer.pkl\", \"rb\") as f:\n",
    "    output_tokenizer = pickle.load(f)\n",
    "    \n",
    "prompt = \"Button in Dark Slate with 'Contact' text\"\n",
    "\n",
    "input_seq = input_tokenizer.texts_to_sequences([prompt])\n",
    "input_seq = pad_sequences(input_seq, maxlen=max_encoder_len, padding='post')\n",
    "\n",
    "encoder_outputs = encoder_model.predict(input_seq)\n",
    "output_seq = [output_tokenizer.word_index[\"<Start>\"]]\n",
    "\n",
    "for _ in range(max_decoder_len):\n",
    "    decoder_input = np.array([output_seq[-1]]).reshape(1, 1)  \n",
    "    predictions = decoder_model.predict([decoder_input, encoder_outputs])\n",
    "    \n",
    "    next_token = np.argmax(predictions[0, 0, :])  \n",
    "    output_seq.append(next_token)\n",
    "    \n",
    "    if next_token == output_tokenizer.word_index[\"<End>\"]:\n",
    "        break\n",
    "\n",
    "decoded_words = [output_tokenizer.index_word.get(i, '') for i in output_seq[1:]]  \n",
    "decoded_sentence = ' '.join(decoded_words)\n",
    "print(decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08acfd45",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- Encoder Decoder works not too good because of teachers forcing\n",
    "- Made a mistake while training which results in 91 val_accuracy -> Problem was not to map the pad to 0 correctly so the model didnt ignored and learned to predict them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
